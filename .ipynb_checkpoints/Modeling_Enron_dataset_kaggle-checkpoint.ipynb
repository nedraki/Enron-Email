{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "df120470-b6a6-d1c5-2ecc-b7129b2466dd"
   },
   "outputs": [],
   "source": [
    "import os, sys, email,re\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns; sns.set_style('whitegrid')\n",
    "# import wordcloud\n",
    "\n",
    "# # Network analysis\n",
    "# import networkx as nx\n",
    "\n",
    "# # NLP\n",
    "# from nltk.tokenize.regexp import RegexpTokenizer\n",
    "\n",
    "# from subprocess import check_output\n",
    "\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "# from sklearn.cluster import KMeans\n",
    "# from sklearn.decomposition import PCA\n",
    "# from sklearn.lda import LDA\n",
    "# from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# import gensim\n",
    "# from gensim import corpora\n",
    "# from nltk.corpus import stopwords \n",
    "# from nltk.stem.wordnet import WordNetLemmatizer\n",
    "# import string\n",
    "# from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "2a6492f3-ffbc-93d2-1135-1f3e23088f8a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(517401, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>allen-p/_sent_mail/1.</td>\n",
       "      <td>Message-ID: &lt;18782981.1075855378110.JavaMail.e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>allen-p/_sent_mail/10.</td>\n",
       "      <td>Message-ID: &lt;15464986.1075855378456.JavaMail.e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>allen-p/_sent_mail/100.</td>\n",
       "      <td>Message-ID: &lt;24216240.1075855687451.JavaMail.e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>allen-p/_sent_mail/1000.</td>\n",
       "      <td>Message-ID: &lt;13505866.1075863688222.JavaMail.e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>allen-p/_sent_mail/1001.</td>\n",
       "      <td>Message-ID: &lt;30922949.1075863688243.JavaMail.e...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       file                                            message\n",
       "0     allen-p/_sent_mail/1.  Message-ID: <18782981.1075855378110.JavaMail.e...\n",
       "1    allen-p/_sent_mail/10.  Message-ID: <15464986.1075855378456.JavaMail.e...\n",
       "2   allen-p/_sent_mail/100.  Message-ID: <24216240.1075855687451.JavaMail.e...\n",
       "3  allen-p/_sent_mail/1000.  Message-ID: <13505866.1075863688222.JavaMail.e...\n",
       "4  allen-p/_sent_mail/1001.  Message-ID: <30922949.1075863688243.JavaMail.e..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the data into a DataFrame\n",
    "emails_df = pd.read_csv('Enron_Kaggle_dataset/emails.csv')\n",
    "print(emails_df.shape)\n",
    "emails_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "c55ba68d-6955-0d73-3aca-9f33bfc870ab"
   },
   "outputs": [],
   "source": [
    "## Helper functions\n",
    "def get_text_from_email(msg):\n",
    "    '''To get the content from email objects'''\n",
    "    parts = []\n",
    "    for part in msg.walk():\n",
    "        if part.get_content_type() == 'text/plain':\n",
    "            parts.append( part.get_payload() )\n",
    "    return ''.join(parts)\n",
    "\n",
    "def split_email_addresses(line):\n",
    "    '''To separate multiple email addresses'''\n",
    "    if line:\n",
    "        addrs = line.split(',')\n",
    "        addrs = frozenset(map(lambda x: x.strip(), addrs))\n",
    "    else:\n",
    "        addrs = None\n",
    "    return addrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "4d7327aa-adfb-aee3-d4f1-e6631a7ddc9e"
   },
   "outputs": [],
   "source": [
    "# Parse the emails into a list email objects\n",
    "\n",
    "messages = list(map(email.message_from_string, emails_df['message']))\n",
    "emails_df.drop('message', axis=1, inplace=True)\n",
    "\n",
    "# Get fields from parsed email objects\n",
    "keys = messages[0].keys()\n",
    "for key in keys:\n",
    "    emails_df[key] = [doc[key] for doc in messages]\n",
    "    \n",
    "# Parse content from emails\n",
    "emails_df['content'] = list(map(get_text_from_email, messages))\n",
    "\n",
    "# Split multiple email addresses\n",
    "emails_df['From'] = emails_df['From'].map(split_email_addresses)\n",
    "emails_df['To'] = emails_df['To'].map(split_email_addresses)\n",
    "\n",
    "# Extract the root of 'file' as 'user'\n",
    "emails_df['user'] = emails_df['file'].map(lambda x:x.split('/')[0])\n",
    "del messages\n",
    "\n",
    "emails_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "1d641bec-4605-75c9-da9e-3d8cc4a64dfa"
   },
   "outputs": [],
   "source": [
    "# Set index and drop columns with two few values\n",
    "\n",
    "emails_df = emails_df.set_index('Message-ID')\\\n",
    "    .drop(['file', 'Mime-Version', 'Content-Type', 'Content-Transfer-Encoding'], axis=1)\n",
    "\n",
    "# Parse datetime\n",
    "emails_df['Date'] = pd.to_datetime(emails_df['Date'], infer_datetime_format=True)\n",
    "emails_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emails_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "6e42e883-9417-a7db-c77e-9b59e78b3c52"
   },
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "    stop = set(stopwords.words('english'))\n",
    "    stop.update((\"to\",\"cc\",\"subject\",\"http\",\"from\",\"sent\",\"aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\"))\n",
    "    exclude = set(string.punctuation) \n",
    "    lemma = WordNetLemmatizer()\n",
    "    porter= PorterStemmer()\n",
    "    \n",
    "    text=text.rstrip()\n",
    "    text = re.sub(r'[^a-zA-Z]', ' ', text)\n",
    "    stop_free = \" \".join([i for i in text.lower().split() if((i not in stop) and (not i.isdigit()))])\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "    #stem = \" \".join(porter.stem(token) for token in normalized.split())\n",
    "    \n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "5fc9545f-2207-8d5e-8380-58997549c84b"
   },
   "outputs": [],
   "source": [
    "analysis_df=emails_df[['From', 'To', 'Date','content']].dropna().copy()\n",
    "analysis_df = analysis_df.loc[analysis_df['To'].map(len) == 1]\n",
    "sub_df=analysis_df.sample(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b62b77a3-fc79-b883-8eb3-67c4a77baed8"
   },
   "outputs": [],
   "source": [
    "#sub_df[\"content\"]=sub_df[\"content\"].map(clean)\n",
    "text_clean=[]\n",
    "for text in sub_df['content']:\n",
    "    text_clean.append(clean(text).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "89ab4b9a-9132-6fea-17b7-135f1dc00a7c"
   },
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(text_clean)\n",
    "text_term_matrix = [dictionary.doc2bow(text) for text in text_clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "16714ab8-100b-93da-5ef9-1991e59a0668"
   },
   "outputs": [],
   "source": [
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "ldamodel = Lda(text_term_matrix, num_topics=4, id2word = dictionary, passes=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "4a22fdc9-7bc1-aabe-686e-3c01d638978d"
   },
   "outputs": [],
   "source": [
    "eng_stopwords = set(stopwords.words('english'))\n",
    "def clean_text(text):\n",
    "    #text = BeautifulSoup(text, 'html.parser').get_text()\n",
    "    text = re.sub(r'[^a-zA-Z]', ' ', text)\n",
    "    words = text.lower().split()\n",
    "    words = [w for w in words if w not in eng_stopwords]\n",
    "    return ' '.join(words)\n",
    "\n",
    "analysis_df[\"clean_content\"]=analysis_df.content.apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "a2a311d8-f00a-f085-633b-dc0a64ad32d1"
   },
   "outputs": [],
   "source": [
    "wordvector = TfidfVectorizer(analyzer='word', stop_words='english', max_df=0.4, min_df=5)\n",
    "short_analysis=analysis_df.sample(5000)\n",
    "wordvector_fit = wordvector.fit_transform(short_analysis.clean_content)\n",
    "feature = wordvector.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "43ee9d65-133c-c4ba-f6cf-37f1c4571ca6"
   },
   "outputs": [],
   "source": [
    "N = 4\n",
    "clf = KMeans(n_clusters=N, \n",
    "            max_iter=50, \n",
    "            init='k-means++', \n",
    "            n_init=1)\n",
    "labels = clf.fit_predict(wordvector_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "ce515e80-b3a2-183a-f20b-ab3c4360c015"
   },
   "outputs": [],
   "source": [
    "wordvector_fit_2d = wordvector_fit.todense()\n",
    "pca = PCA(n_components=2).fit(wordvector_fit_2d)\n",
    "datapoint = pca.transform(wordvector_fit_2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "eabec3f2-37aa-6a05-cf33-5aef39126c16"
   },
   "outputs": [],
   "source": [
    "label = [\"#e05f14\", \"#e0dc14\", \"#2fe014\", \"#14d2e0\"]\n",
    "color = [label[i] for i in labels]\n",
    "plt.scatter(datapoint[:, 0], datapoint[:, 1], c=color)\n",
    "\n",
    "centroids = clf.cluster_centers_\n",
    "centroidpoint = pca.transform(centroids)\n",
    "plt.scatter(centroidpoint[:, 0], centroidpoint[:, 1], marker='^', s=150, c='#000000')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "9cf4d20f-23e8-b18a-937c-850a57be9fcc"
   },
   "outputs": [],
   "source": [
    "#print(ldamodel.print_topics(num_topics=4, num_words=10))\n",
    "print([(0, '0.012*\"enron\" + 0.012*\"deal\" + 0.010*\"agreement\" + 0.008*\"change\" + 0.008*\"contract\" + 0.008*\"corp\" + 0.007*\"fax\" + 0.005*\"houston\" + 0.005*\"date\" + 0.005*\"america\"'), (1, '0.005*\"message\" + 0.005*\"origin\" + 0.004*\"pleas\" + 0.004*\"email\" + 0.004*\"thank\" + 0.003*\"attach\" + 0.003*\"file\" + 0.003*\"copy\" + 0.003*\"inform\" + 0.003*\"receive\"'), (2, '0.015*\"thank\" + 0.008*\"call\" + 0.005*\"time\" + 0.004*\"meet\" + 0.003*\"look\" + 0.003*\"week\" + 0.003*\"day\" + 0.003*\"lunch\" + 0.003*\"talk\" + 0.003*\"hello\"'), (3, '0.016*\"market\" + 0.009*\"gas\" + 0.008*\"price\" + 0.005*\"power\" + 0.004*\"company\" + 0.004*\"energy\" + 0.003*\"business\" + 0.003*\"service\" + 0.003*\"manage\" + 0.003*\"fare\"')])"
   ]
  }
 ],
 "metadata": {
  "_change_revision": 0,
  "_is_fork": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
